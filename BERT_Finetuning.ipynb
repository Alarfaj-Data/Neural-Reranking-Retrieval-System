{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HEkSMyRunAj-"
   },
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rNOxyuD7nc1j"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler, TensorDataset\n",
    "from transformers import (AdamW, BertForSequenceClassification, BertTokenizer,\n",
    "                          get_linear_schedule_with_warmup)\n",
    "from tqdm import tqdm\n",
    "from google.colab import drive\n",
    "\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Set directories\n",
    "data_dir = '/content/drive/MyDrive/MSMARCO/'  # Data directory\n",
    "main_dir = '/content/drive/MyDrive/'  # Main directory\n",
    "\n",
    "# Check for GPU\n",
    "print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5h90U53sorjE"
   },
   "outputs": [],
   "source": [
    "# Data preprocess block, this takes MSMARCO dataset which only has positive labels (qrels)\n",
    "# and assigns a random document to each query from the collection to be used as a negative label\n",
    "# It also adds query text and document text so this final file can be used without referencing the huge collection file\n",
    "# in the end we have a file with these fields columns=['qid', '0', 'docid', 'label', 'query_text','doc_text'])\n",
    "\n",
    "\n",
    "# Load datasets\n",
    "train_queries = pd.read_csv(os.path.join(data_dir, 'queries.doctrain.tsv'), sep='\\t', header=None, names=['qid', 'query_text'])\n",
    "train_queries_index = train_queries.set_index('qid')\n",
    "\n",
    "train_relations = pd.read_csv(os.path.join(data_dir, 'msmarco-doctrain-qrels.tsv'), sep=' ', header=None, names=['qid', '0', 'docid', 'label'])\n",
    "\n",
    "val_queries = pd.read_csv(os.path.join(data_dir, 'queries.docdev.tsv'), sep='\\t', header=None, names=['qid', 'query_text'])\n",
    "val_queries_index = val_queries.set_index('qid')\n",
    "\n",
    "val_relations = pd.read_csv(os.path.join(data_dir, 'msmarco-docdev-qrels.tsv'), sep=' ', header=None, names=['qid', '0', 'docid', 'label'])\n",
    "\n",
    "lookup = pd.read_csv(os.path.join(data_dir, 'msmarco-docs-lookup.tsv'), sep='\\t', header=None, names=['docid', 'trec_offset', 'tsv_offset'], usecols=['docid', 'trec_offset', 'tsv_offset'])\n",
    "lookup_list = lookup.values.tolist()\n",
    "\n",
    "collection = pd.read_csv(os.path.join(data_dir, 'msmarco-docs.tsv'), sep='\\t', header=None, names=['docid', 'url', 'title', 'doc_text'], index_col='docid')\n",
    "\n",
    "\n",
    "def negative_samples(queries, relations, queries_index):\n",
    "    negative_samples_list = []\n",
    "    for i in tqdm(range(len(queries))):\n",
    "        tmp_list = [queries[i][0], '0', random.sample(lookup_list, 1)[0][0], 0, queries[i][1]]\n",
    "        negative_samples_list.append(tmp_list)\n",
    "    for k in tqdm(range(len(relations))):\n",
    "        query_text = queries_index.loc[relations[k][0]].query_text\n",
    "        relations[k].append(query_text)\n",
    "    return negative_samples_list + relations\n",
    "\n",
    "\n",
    "def add_doc_text(dataset):\n",
    "    for i in tqdm(range(len(dataset))):\n",
    "        docid = dataset[i][2]\n",
    "        document = collection.loc[docid].doc_text\n",
    "        dataset[i].append(document)\n",
    "    return pd.DataFrame(dataset, columns=['qid', '0', 'docid', 'label', 'query_text', 'doc_text'])\n",
    "\n",
    "train_dataset = add_doc_text(negative_samples(train_queries.values.tolist(), train_relations.values.tolist(), train_queries_index))\n",
    "val_dataset = add_doc_text(negative_samples(val_queries.values.tolist(), val_relations.values.tolist(), val_queries_index))\n",
    "train_dataset.to_csv(os.path.join(data_dir, 'train_dataset.csv'))\n",
    "val_dataset.to_csv(os.path.join(data_dir, 'val_dataset.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nYLPpbeYSXDI"
   },
   "outputs": [],
   "source": [
    "# This block is to be used when a preprocessed dataset is already created (that has positive and negative labels)\n",
    "\n",
    "\n",
    "train_dataset = pd.read_csv(os.path.join(data_dir, 'train_dataset.csv'), header=0,\n",
    "                          names=['qid', '0', 'docid', 'label', 'query_text', 'doc_text'])\n",
    "\n",
    "val_dataset = pd.read_csv(os.path.join(data_dir, 'val_dataset.csv'), header=0,\n",
    "                          names=['qid', '0', 'docid', 'label', 'query_text', 'doc_text'])                         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ISV3U44BcOhH"
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') # Tokeizer initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q4IurB99cvb_"
   },
   "outputs": [],
   "source": [
    "# This function handels the toknizer and returns TensorDataset to be fed to the model\n",
    " \n",
    "    \n",
    "def tokenize(dataset, label):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    token_type_ids = []\n",
    "\n",
    "    for i in tqdm(range(len(dataset))):\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            str(dataset[i][4]),  # Query\n",
    "            str(dataset[i][5]),  # Document\n",
    "            add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
    "            max_length=512,  # Pad & truncate all sentences.\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,  # Construct attn. masks.\n",
    "            truncation='only_second',\n",
    "            return_tensors='pt',  # Return pytorch tensors.\n",
    "            return_token_type_ids=True  # To differentiate query sequence from document sequence\n",
    "        )\n",
    "\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "        token_type_ids.append(encoded_dict['token_type_ids'])\n",
    "\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    token_type_ids = torch.cat(token_type_ids, dim=0)\n",
    "    labels = torch.tensor(label)\n",
    "\n",
    "    return TensorDataset(input_ids, attention_masks, token_type_ids, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9SjUgyM9im1T"
   },
   "outputs": [],
   "source": [
    "# This block calls the tokenizer defined above and saves the tensor datasets to .pt files,\n",
    "# so we do no have to go through this process every time we run the model\n",
    "\n",
    "\n",
    "train_dataset_list = train_dataset.values.tolist() # Converting the dataframe to list (more efficient)\n",
    "val_dataset_list = val_dataset.values.tolist() # Converting the dataframe to list (more efficient)\n",
    "train_Labels = train_dataset.label.values # Extracting labels\n",
    "val_labels = val_dataset.label.values # Extracting labels\n",
    "\n",
    "train_dataset_tensor = tokenize(train_dataset_list, train_Labels) # Tokenization and creation of TensorDataset\n",
    "val_dataset_tensor = tokenize(val_dataset_list, val_labels) # Tokenization and creation of TensorDataset\n",
    "\n",
    "torch.save(train_dataset_tensor, os.path.join(main_dir, 'train_dataset_tensor.pt') # Saving TensorDataset to disk  \n",
    "torch.save(val_dataset_tensor, os.path.join(main_dir, 'val_dataset_tensor.pt') # Saving TensorDataset to disk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block is to be used when a TensorDataset is already created and saved to disk\n",
    "\n",
    "train_dataset_tensor = torch.load('/content/drive/MyDrive/train_dataset_tensor.pt') # Training dataset, Tokenized and converted to tensors ..  Type is TensorDataset\n",
    "val_dataset_tensor = torch.load('/content/drive/MyDrive/val_dataset_tensor.pt') # Validation dataset, Tokenized and converted to tensors ..  Type is TensorDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IMsKeM-btxe6"
   },
   "outputs": [],
   "source": [
    "# Define dataloaders\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset_tensor,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset_tensor), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset_tensor, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset_tensor), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nxwAwr-awCYe"
   },
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels = 2)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MLFzvaWJwyD6"
   },
   "outputs": [],
   "source": [
    "# This block defines the optimizer and the scheduler\n",
    "\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=3e-6)\n",
    "\n",
    "epochs = 2\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 10000,\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XQeHCKr3eR0k"
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "seed_val = 50\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "training_stats = []\n",
    "\n",
    "for epoch_i in range(epochs):\n",
    "    print(f\"\\n======== Epoch {epoch_i + 1} / {epochs} ========\")\n",
    "    print(\"Training...\")\n",
    "\n",
    "    total_train_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if step % 40 == 0 and step != 0:\n",
    "            elapsed = str(datetime.timedelta(seconds=int(time.time() - t0)))\n",
    "            print(f'  Batch {step}  of  {len(train_dataloader)}.    Elapsed: {elapsed}.')\n",
    "\n",
    "        t_input_ids, t_input_mask, t_token_type_ids, t_labels = (b.to(device) for b in batch)\n",
    "        model.zero_grad()\n",
    "        result = model(t_input_ids, token_type_ids=t_token_type_ids, attention_mask=t_input_mask, labels=t_labels, return_dict=True)\n",
    "\n",
    "        loss = result.loss\n",
    "        total_train_loss += loss.item() # Summing training loss to calculate AVG\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        #L2 regularization\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step() # Update the learning rate.\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader) # Calculate the average loss over all of the batches.\n",
    "    print(f\"\\n  Average training loss: {avg_train_loss:.2f}\")\n",
    "\n",
    "    print(\"\\nRunning Validation...\")\n",
    "    t0 = time.time()\n",
    "    model.eval()\n",
    "    total_eval_loss = 0\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        v_input_ids, v_input_mask, v_token_type_ids, v_labels = (b.to(device) for b in batch)\n",
    "        with torch.no_grad():\n",
    "            result = model(v_input_ids, token_type_ids=v_token_type_ids, attention_mask=v_input_mask, labels=v_labels, return_dict=True)\n",
    "        total_eval_loss += result.loss.item()\n",
    "\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    print(f\"  Validation Loss: {avg_val_loss:.2f}\")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pWb6XiMz4Dj7"
   },
   "outputs": [],
   "source": [
    "# This block saves the finetuned model\n",
    "\n",
    "output_dir = '/content/drive/MyDrive/BERT_checkpoint/'\n",
    "\n",
    "model.save_pretrained(output_dir)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "background_execution": "on",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Copy of BERT Finetuning.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
