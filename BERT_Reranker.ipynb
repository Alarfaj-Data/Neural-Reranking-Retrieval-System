{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wjaCjJYN8-ax"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "print('The GPU Assigned is:', torch.cuda.get_device_name(0)) # Tesla P100-PCIE-16GB is really good, if assigned a different GPU, delete the runtime, and connect again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FCVdijuj6puv"
   },
   "outputs": [],
   "source": [
    "%pip install transformers\n",
    "%pip install python-terrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vLsYO6QTLPxj"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset, SequentialSampler\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from google.colab import drive\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import os\n",
    "import math\n",
    "import csv\n",
    "import pyterrier as pt\n",
    "from pyterrier.measures import *\n",
    "if not pt.started():\n",
    "  pt.init()\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "data_dir = '/content/drive/MyDrive/MSMARCO/'\n",
    "full_index_dir = '/content/drive/MyDrive/Full_Index/'\n",
    "dir = '/content/drive/MyDrive/'\n",
    "small_index_dir = '/content/drive/MyDrive/Passage_Index/'\n",
    "\n",
    "print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dv1pW8W74-6l"
   },
   "outputs": [],
   "source": [
    "#Initilize tokenizer and model\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForSequenceClassification.from_pretrained('/content/drive/MyDrive/checkpoint3/', num_labels = 2) #Finetuned model\n",
    "model.cuda() #Model to GPU\n",
    "model.eval() #Evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FtQEat8Ak5eH"
   },
   "outputs": [],
   "source": [
    "def tokenize(DS):\n",
    "  input_ids = []\n",
    "  attention_masks = []\n",
    "  token_type_ids = [] \n",
    "\n",
    "  for i in tqdm(range(len(DS))):\n",
    "\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        str(DS.iloc[i].query), # Query\n",
    "                        str(DS.iloc[i].body), # Document\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 256,           # Maximum length of sequence (I went with BERT maximum)\n",
    "                        padding='max_length',\n",
    "                        return_attention_mask = True,   # Construct attention masks.\n",
    "                        truncation='only_second',\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                        return_token_type_ids=True # To differentiate query sequence from document sequence  \n",
    "                   )\n",
    "      # Add the encoded sentence to the list.    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    token_type_ids.append(encoded_dict['token_type_ids'])\n",
    "\n",
    "\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "token_type_ids = torch.cat(token_type_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "return TensorDataset(input_ids, attention_masks, token_type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-okrMbtoxEyU"
   },
   "outputs": [],
   "source": [
    "msmarco_dataset = pt.get_dataset(\"msmarco_document\")\n",
    "qrels = msmarco_dataset.get_qrels('test')\n",
    "topics = msmarco_dataset.get_topics('test')\n",
    "index = pt.IndexFactory.of(full_index_dir)\n",
    "index2 = pt.IndexFactory.of(passage_index_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xnjhK_nfxZlL"
   },
   "outputs": [],
   "source": [
    "bm25 = pt.BatchRetrieve(index, wmodel=\"BM25\", metadata=[\"title\",\"docno\", \"body\"]) %200 # Retrieve only top 200 docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uiG7cq3QHXQM"
   },
   "outputs": [],
   "source": [
    "# takes a list of candidate results and rerank it\n",
    "def bert(ranked_results, query_list_length, query=None):\n",
    "  ranked_results_tensor = tokenize(ranked_results)\n",
    "\n",
    "  dataloader = DataLoader(\n",
    "              ranked_results_tensor,\n",
    "              sampler = SequentialSampler(ranked_results_tensor),\n",
    "              batch_size=64\n",
    "          )\n",
    "\n",
    "\n",
    "  pred_tensor = []\n",
    "  for batch in dataloader:\n",
    "\n",
    "    b_input_ids = batch[0].to(device)\n",
    "    b_input_mask = batch[1].to(device)\n",
    "    b_token_type_ids = batch[2].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "      result = model(b_input_ids, \n",
    "                      token_type_ids=b_token_type_ids, \n",
    "                      attention_mask=b_input_mask,\n",
    "                      return_dict=True)\n",
    "\n",
    "    logits = result.logits\n",
    "    pred_tensor.append(logits)\n",
    "  pred = torch.cat(pred_tensor, 0)  \n",
    "\n",
    "  prob = torch.sigmoid(pred) # to get probabilities from logits (Since we only have two labels, I used sigmoid)\n",
    "\n",
    "  # deletes first column which has the probability of not being relevant (we care about the probability of being relevant only)\n",
    "  prob_final = np.delete(prob.cpu().numpy(), 0, 1).flatten() \n",
    "\n",
    "  # To get final classification where 1 is relevant and 0 is irrelevant (We do not care for this since this not a classifier)\n",
    "  #Classification = np.argmax(prob.cpu().numpy(), axis=1).flatten() \n",
    "\n",
    "  for i in range(len(ranked_results)):\n",
    "    ranked_results.iloc[i, ranked_results.columns.get_loc('score')] = prob_final[i] # Assigning scores to docs\n",
    "\n",
    "  ranked_results_list = []\n",
    "  if query is not None:\n",
    "    for i in range(query_list_length):\n",
    "      df = ranked_results[ranked_results['qid'] == query.iloc[i].qid] # Extracting docs for each query to rank them\n",
    "      df = df.sort_values(by=['score'], ascending=False) # Sorting by score\n",
    "      df = df.values.tolist()\n",
    "      for p in range(len(df)):\n",
    "        df[p][5] = p # Adding the rank\n",
    "      ranked_results_list.append(df)\n",
    "    ranked_results_list = [x for xs in ranked_results_list for x in xs] # Flattening the list\n",
    "    ranked_results = pd.DataFrame(ranked_results_list, columns=['qid', 'docid', 'title', 'docno', 'body', 'rank', 'score', 'query'])\n",
    "  else:\n",
    "    # If the function get a single query instead of a dataframe \n",
    "      ranked_results = ranked_results.sort_values(by=['score'], ascending=False)\n",
    "      for i in range(len(ranked_results)):\n",
    "        ranked_results.iloc[i, ranked_results.columns.get_loc('rank')] = i\n",
    "      ranked_results = ranked_results.reset_index(drop=True)\n",
    "\n",
    "  return ranked_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xKW8rtLB_bWg"
   },
   "outputs": [],
   "source": [
    "# Returns a tuple of a token and raw count\n",
    "def get_tokens(index2, docno):\n",
    "    token_list = []\n",
    "    termslist = []\n",
    "    docid = index2.getMetaIndex().getDocument(\"docno\", docno) # Returns docid\n",
    "    pointer = index2.getDocumentIndex().getDocumentEntry(docid) # Returns DocumentIndexEntry which can be used as a pointer \n",
    "    iterator = index2.getDirectIndex().getPostings(pointer) # Returns the posting iterator\n",
    "    for p in iterator:\n",
    "        raw_count = p.getFrequency()\n",
    "        termid = p.getId() # Returns term id in the lexicon \n",
    "        term = index2.getLexicon().getLexiconEntry(termid).getKey() # Returns the key (the word itself)\n",
    "        token_list.append([term, raw_count])\n",
    "        termslist.append(term)\n",
    "    return token_list, termslist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZPkMtYqZr0z5"
   },
   "outputs": [],
   "source": [
    "# Takes the top K doc list, query text, and k (number of tokens to be added to the query) \n",
    "# Returns the new expanded query\n",
    "def PRF(PRF_List, query, k, qid=None):\n",
    "    # Parameters:\n",
    "    # PRF_List: The Top K docs dataframe\n",
    "    # query: Query text\n",
    "    # k: The number of tokens to be added to the expanded query\n",
    "    # qid: Query ID\n",
    "\n",
    "\n",
    "    all_tokens = [] # List of tokens\n",
    "    all_terms = [] # List of term (Vocabulary)\n",
    "    for i in range(len(PRF_List)):\n",
    "      all_tknz, all_trms = get_tokens(index2, PRF_List.iloc[i].docno) # This gets the list of tokens from the the document \n",
    "      all_tokens.append(all_tknz) # This list has tokens and raw count\n",
    "      all_terms.append(all_trms) # This list has just tokens, I will convert it to a list of terms using set()\n",
    "    flatten_tokenz_list = [x for xs in all_tokens for x in xs] # Flatten to have all tokens from all docs in one list\n",
    "    flatten_terms_list = [x for xs in all_terms for x in xs] # Flatten to have all terms (Vocab) from all docs in one list \n",
    "    flatten_terms_list = set(flatten_terms_list) # I used set here to remove duplicate terms\n",
    "\n",
    "    list_of_terms_with_raw_count_number_of_docs = [] # List of all terms with frequency in the top K documents and number of document that has this term\n",
    "    for i in flatten_terms_list:\n",
    "      total_raw_count = 0\n",
    "      number_of_docs_that_has_term = 0\n",
    "      for sublist in flatten_tokenz_list:\n",
    "        if sublist[0] == i:\n",
    "          number_of_docs_that_has_term  = number_of_docs_that_has_term + 1 # Getting the number of docs that has this term\n",
    "          total_raw_count = total_raw_count + sublist[1] # Summing raw count for this token from all docs that has it\n",
    "      list_of_terms_with_raw_count_number_of_docs.append((i, total_raw_count, number_of_docs_that_has_term))\n",
    "\n",
    "    tokens_score = []\n",
    "    numberOfDocuments = index2.getCollectionStatistics().getNumberOfDocuments()\n",
    "    for i in list_of_terms_with_raw_count_number_of_docs:\n",
    "      if i[2] >= 3: # if token appear in 3 of the top 5 docuemnts\n",
    "        try:\n",
    "          CollFreq = index2.getLexicon()[i[0]].getFrequency()\n",
    "          f = CollFreq / numberOfDocuments\n",
    "          score = -math.log2(1/(1 + f)) - (i[1] * math.log2(f/(1 + f))) # Bo1 \n",
    "          tokens_score.append((i[0], score))\n",
    "        except:\n",
    "          print(i[0])\n",
    "\n",
    "    sorted_by_score = sorted(tokens_score, key=lambda tup: tup[1], reverse=True) # Sorting tuples by their score\n",
    "    tokens_to_be_added = [] # tokens to be added to the original query\n",
    "    s = 0\n",
    "    while s != -1:\n",
    "      if len(sorted_by_score) == s:\n",
    "        break\n",
    "    # if the current token is not in the tokens_to_be_added list, it will be added\n",
    "      tokens_to_be_added.append(sorted_by_score[s][0])\n",
    "      #print(sorted_by_score[s][1])\n",
    "      s=s+1\n",
    "      if len(tokens_to_be_added) == k:\n",
    "        break\n",
    "    top_k_words = \" \".join(tokens_to_be_added) # List of tokens to a string \n",
    "    #return tokens_to_be_added\n",
    "    if qid is not None:\n",
    "      return [qid, query+\" \"+top_k_words] # The new expanded query\n",
    "    else:\n",
    "      return query+\" \"+top_k_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "018NtMnOyaY9"
   },
   "outputs": [],
   "source": [
    "class neural_reranking():\n",
    "  def transform(query):\n",
    "    query_list_length = 1\n",
    "    if isinstance(query, pd.DataFrame):\n",
    "      query_list_length = len(query.index)\n",
    "      reranked_list = bert(bm25(query), query_list_length, query) # Candidate list by BM25, reranking by BERT\n",
    "    else:\n",
    "      reranked_list = bert(bm25(query), query_list_length) # Candidate list by BM25, reranking by BERT\n",
    "\n",
    "    return reranked_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-xF3mBfddS2v"
   },
   "outputs": [],
   "source": [
    "class neural_reranking_PRF():\n",
    "  def transform(query):\n",
    "    K = 5 # Top K docs to choose token from \n",
    "    k = 10 # Top k tokens to be added to the query\n",
    "    query_list_length = 1\n",
    "    if isinstance(query, pd.DataFrame):\n",
    "      query_list_length = len(query.index)\n",
    "      reranked_list_PRF = bert(bm25(query), query_list_length) # Candidate list by BM25, reranking by BERT\n",
    "      # List of docs to be sent to PRF\n",
    "      PRF_List = []\n",
    "      for i in range(query_list_length):\n",
    "        Query_Docs = reranked_list_PRF[reranked_list_PRF['qid'] == str(query.iloc[i].qid)] # Since BERT returned all queries and their docs in one list, I had to seperate them by query\n",
    "        Top_K_Docs = Query_Docs.head(K) #Getting top K documetns for PRF for each query\n",
    "        Expanded_Q = PRF(Top_K_Docs, query.iloc[i].query, k, qid=topics.iloc[i].qid) # These parameters are explained in PRF function\n",
    "        reranked_list_PRF.loc[reranked_list_PRF['qid'] == str(query.iloc[i].qid), \"query\"] = Expanded_Q[1] # replace every query with expanded query\n",
    "\n",
    "      # reranking by BERT using the expanded query\n",
    "      reranked_list_PRF = bert(reranked_list_PRF, query_list_length, query)\n",
    "\n",
    "    return reranked_list_PRF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JSw22GHCBAaU"
   },
   "outputs": [],
   "source": [
    "Results = neural_reranking.transform() # Takes a query in text format or dataframe (No PRF)\n",
    "Results = neural_reranking_PRF.transform() # Takes a query in text format or dataframe (With PRF)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Copy of BERT Reranker.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
