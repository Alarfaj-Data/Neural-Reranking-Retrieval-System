{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "omC6KJmE_k4z",
   "metadata": {
    "id": "omC6KJmE_k4z"
   },
   "outputs": [],
   "source": [
    "# Dask is more efficient than pandas\n",
    "!pip install tqdm spacy dask[dataframe] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a82f17",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "63a82f17",
    "outputId": "a30106bb-f57b-49e7-cdcb-e098bdb5092a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Constants\n",
    "DATA_DIR = '/content/drive/MyDrive/MSMARCO/'\n",
    "COLLECTION_FILE = 'msmarco-docs.tsv'\n",
    "OUTPUT_FILE = 'Passage_Collection.csv'\n",
    "MAX_TOKENS = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1e4fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer():\n",
    "    \"\"\"Loads the spaCy tokenizer with specific components excluded.\"\"\"\n",
    "    nlp = spacy.load(\"en_core_web_sm\", exclude=[\n",
    "        \"ner\", \"tagger\", \"parser\", \"lemmatizer\", \"textcat\", \"attribute_ruler\"\n",
    "    ])\n",
    "    nlp.max_length = 2500000\n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f290e041",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_collection(data_dir, filename):\n",
    "    \"\"\"Reads the collection file into a DataFrame.\"\"\"\n",
    "    file_path = os.path.join(data_dir, filename)\n",
    "    collection = dd.read_csv(\n",
    "        file_path, sep='\\t', header=None,\n",
    "        usecols=[0, 3], names=['docid', 'doc_text']\n",
    "    ).compute()\n",
    "    return collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vTBgJkph-TCD",
   "metadata": {
    "id": "vTBgJkph-TCD"
   },
   "outputs": [],
   "source": [
    "def process_collection(collection, tokenizer, max_tokens):\n",
    "    \"\"\"Processes the collection to extract a limited number of tokens.\"\"\"\n",
    "    passage_collection = []\n",
    "    for i in tqdm(range(len(collection))):\n",
    "        new_doc_tokens = []\n",
    "        count = 0\n",
    "        tokens = tokenizer(str(collection.iloc[i].doc_text)) # tokenize to get a list of tokens \n",
    "        for token in tokens:\n",
    "            if count == max_tokens:\n",
    "                break\n",
    "            if not token.is_space:\n",
    "                new_doc_tokens.append(token.text) # add tokens until reaching 500\n",
    "                count += 1\n",
    "        passage_collection.append([collection.iloc[i].docid, ' '.join(new_doc_tokens)])\n",
    "    return passage_collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QMkgKCQbQU3H",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "QMkgKCQbQU3H",
    "outputId": "56de9619-f7d4-4424-e8f8-bebb5de45423"
   },
   "outputs": [],
   "source": [
    "def write_to_csv(data, data_dir, filename):\n",
    "    \"\"\"Writes the processed data to a CSV file.\"\"\"\n",
    "    file_path = os.path.join(data_dir, filename)\n",
    "    columns = ['docid', 'doc_text']\n",
    "    with open(file_path, 'w', encoding=\"utf8\", newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(columns)\n",
    "        writer.writerows(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6200575d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = load_tokenizer()\n",
    "collection = read_collection(DATA_DIR, COLLECTION_FILE)\n",
    "passage_collection = process_collection(collection, tokenizer, MAX_TOKENS)\n",
    "write_to_csv(passage_collection, DATA_DIR, OUTPUT_FILE)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "background_execution": "on",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Passage Collection.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
