{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HEkSMyRunAj-"
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rNOxyuD7nc1j"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, RandomSampler, SequentialSampler\n",
    "from transformers import LongformerTokenizer, LongformerForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from google.colab import drive\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import time\n",
    "import spacy\n",
    "import datetime\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\", exclude=[\"ner\", \"tagger\", \"parser\", \"lemmatizer\", \"textcat\", \"attribute_ruler\"])\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "data_dir = '/content/drive/MyDrive/MSMARCO/' # Data directory\n",
    "dir = '/content/drive/MyDrive/' # Main directory\n",
    "print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "device = torch.device(\"cuda\") # To run the model and process the tensors on GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5h90U53sorjE"
   },
   "outputs": [],
   "source": [
    "# Data preprocess block, this takes MSMARCO dataset which only has positive labels (qrels)\n",
    "# and assigns a random document to each query from the collection to be used as a negative label\n",
    "# It also adds query text and document text so this final file can be used without referencing the huge collection file\n",
    "# in the end we have a file with these fields columns=['qid', '0', 'docid', 'label', 'query_text','doc_text'])\n",
    "\n",
    "train_queries = pd.read_csv(os.path.join(data_dir, 'queries.doctrain.tsv'),\n",
    "                                   sep='\\t', header=None, names=['qid', 'query_text'])\n",
    "train_queries_index = train_queries.set_index('qid')\n",
    "\n",
    "train_relations = pd.read_csv(os.path.join(data_dir, 'msmarco-doctrain-qrels.tsv'),\n",
    "                                     sep=' ', header=None, names=['qid', '0', 'docid', 'label'])\n",
    "        \n",
    "val_queries = pd.read_csv(os.path.join(data_dir, 'queries.docdev.tsv'),\n",
    "                                   sep='\\t', header=None, names=['qid', 'query_text'])\n",
    "val_queries_index = val_queries.set_index('qid')\n",
    "\n",
    "val_relations = pd.read_csv(os.path.join(data_dir, 'msmarco-docdev-qrels.tsv'),\n",
    "                                     sep=' ', header=None, names=['qid', '0', 'docid', 'label'])\n",
    "\n",
    "lookup = pd.read_csv(os.path.join(data_dir,'msmarco-docs-lookup.tsv'),\n",
    "                                        sep='\\t', header=None,\n",
    "                                        names=['docid', 'trec_offset', 'tsv_offset'],\n",
    "                                        usecols=['docid', 'trec_offset', 'tsv_offset'])\n",
    "lookup_list = lookup.values.tolist()\n",
    "\n",
    "collection = pd.read_csv(os.path.join(data_dir, 'msmarco-docs.tsv'),\n",
    "                       sep='\\t', header=None,\n",
    "                       names=['docid', 'url', 'title', 'doc_text'], index_col='docid',)\n",
    "\n",
    "def negative_samples(queries, relations, queries_index):\n",
    "  Negative_Samples_List = []\n",
    "  for i in tqdm(range(len(queries))):\n",
    "    tmp_list = []\n",
    "    tmp_list.append(queries[i][0])\n",
    "    tmp_list.append('0')\n",
    "    docid_n = random.sample(lookup_list, 1)[0][0]\n",
    "    tmp_list.append(docid_n)\n",
    "    tmp_list.append(0)\n",
    "    tmp_list.append(queries[i][1])\n",
    "    Negative_Samples_List.append(tmp_list)\n",
    "  for k in tqdm(range(len(relations))):\n",
    "    query_text = queries_index.loc[relations[k][0]].query_text\n",
    "    relations[k].append(query_text)\n",
    "\n",
    "  return Negative_Samples_List+relations\n",
    "\n",
    "def add_doc_text(dataset):\n",
    "  for i in tqdm(range(len(dataset))):\n",
    "    docid = dataset[i][2]\n",
    "    document = collection.loc[docid].doc_text\n",
    "    dataset[i].append(document)\n",
    "  return pd.DataFrame(dataset, columns=['qid', '0', 'docid', 'label', 'query_text','doc_text'])\n",
    "\n",
    "train_dataset = add_doc_text(negative_samples(train_queries.values.tolist(), train_relations.values.tolist(), train_queries_index))\n",
    "val_dataset = add_doc_text(negative_samples(val_queries.values.tolist(), val_relations.values.tolist(), val_queries_index))\n",
    "train_dataset.to_csv(os.path.join(data_dir, 'train_dataset.csv'))\n",
    "val_dataset.to_csv(os.path.join(data_dir, 'val_dataset.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nYLPpbeYSXDI"
   },
   "outputs": [],
   "source": [
    "# This block is to be used when having a ready dataset (that has positive and negative labels)\n",
    "\n",
    "\n",
    "train_dataset = pd.read_csv(os.path.join(data_dir, 'train_dataset.csv'), header=0,\n",
    "                          names=['qid', '0', 'docid', 'label', 'query_text', 'doc_text'])\n",
    "\n",
    "val_dataset = pd.read_csv(os.path.join(data_dir, 'val_dataset.csv'), header=0,\n",
    "                          names=['qid', '0', 'docid', 'label', 'query_text', 'doc_text'])                         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ISV3U44BcOhH"
   },
   "outputs": [],
   "source": [
    "tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096') # Tokeizer initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q4IurB99cvb_"
   },
   "outputs": [],
   "source": [
    "# This function handels the toknizer and returns TensorDataset to be fed to the model\n",
    "\n",
    "def tokenize(DS, Label):\n",
    "  input_ids = []\n",
    "  attention_masks = []\n",
    "  global_attention_mask = [] \n",
    "\n",
    "  for i in tqdm(range(len(DS))):\n",
    "\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        str(DS[i][4]), # Query\n",
    "                        str(DS[i][5]), # Document\n",
    "                        add_special_tokens = True, # Add '<s>' and '</s>'\n",
    "                        max_length = 1024,           # Pad & truncate all sentences.\n",
    "                        padding='max_length',\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        truncation='only_second',\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "      # Add the encoded sentence to the list.    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "\n",
    "    global_attention = None\n",
    "    global_attention = [0] * 1024\n",
    "    range_with_CLS = len(nlp(DS[i][4])) + 1\n",
    "    for i in range(range_with_CLS):\n",
    "      global_attention[i] = 1\n",
    "    global_attention_mask.append(torch.tensor([global_attention]))\n",
    "\n",
    "\n",
    "  \n",
    "  input_ids = torch.cat(input_ids, dim=0)\n",
    "  attention_masks = torch.cat(attention_masks, dim=0)\n",
    "  global_attention_masks = torch.cat(global_attention_mask, dim=0)\n",
    "  labels = torch.tensor(Label)\n",
    "\n",
    "\n",
    "  return TensorDataset(input_ids, attention_masks, labels, global_attention_masks)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9SjUgyM9im1T"
   },
   "outputs": [],
   "source": [
    "# This block calls the tokenizer defined above and saves the tensor datasets to .pt files,\n",
    "# so we do no have to go through this process every time we run the model\n",
    "\n",
    "\n",
    "train_dataset_list = train_dataset.values.tolist() # Converting the dataframe to list (more efficient)\n",
    "val_dataset_list = val_dataset.values.tolist() # Converting the dataframe to list (more efficient)\n",
    "train_Labels = train_dataset.label.values # Extracting labels\n",
    "val_labels = val_dataset.label.values # Extracting labels\n",
    "\n",
    "train_dataset_tensor = tokenize(train_dataset_list, train_Labels) # Tokenization and creation of TensorDataset\n",
    "val_dataset_tensor = tokenize(val_dataset_list, val_labels) # Tokenization and creation of TensorDataset\n",
    "\n",
    "torch.save(train_dataset_tensor, os.path.join(dir, 'train_dataset_tensor.pt') # Saving TensorDataset to disk  \n",
    "torch.save(val_dataset_tensor, os.path.join(dir, 'val_dataset_tensor.pt') # Saving TensorDataset to disk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IMsKeM-btxe6"
   },
   "outputs": [],
   "source": [
    "train_dataset_tensor = torch.load('/content/drive/MyDrive/Longformer_train_dataset_tensor.pt') # Training dataset, Tokenized and converted to tensors ..  Type is TensorDataset\n",
    "val_dataset_tensor = torch.load('/content/drive/MyDrive/Longformer_val_dataset_tensor.pt') # Validation dataset, Tokenized and converted to tensors ..  Type is TensorDataset\n",
    "\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset_tensor,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset_tensor), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset_tensor, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset_tensor), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nxwAwr-awCYe"
   },
   "outputs": [],
   "source": [
    "model = LongformerForSequenceClassification.from_pretrained('allenai/longformer-base-4096', num_labels = 2)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MLFzvaWJwyD6"
   },
   "outputs": [],
   "source": [
    "# This block defines the optimizer and the scheduler\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 3e-6)\n",
    "\n",
    "epochs = 1\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 2500,\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XQeHCKr3eR0k"
   },
   "outputs": [],
   "source": [
    "seed_val = 50\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "training_stats = []\n",
    "\n",
    "total_t0 = time.time()\n",
    "\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    t0 = time.time()\n",
    "    total_train_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every 100 batches.\n",
    "        if step % 100 == 0 and not step == 0:\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "\n",
    "        t_input_ids = batch[0].to(device)\n",
    "        t_input_mask = batch[1].to(device)\n",
    "        t_labels = batch[2].to(device)\n",
    "        t_global = batch[3].to(device)\n",
    "\n",
    "        model.zero_grad()        \n",
    "\n",
    "        result = model(t_input_ids,\n",
    "                       attention_mask=t_input_mask, \n",
    "                       labels=t_labels,\n",
    "                       global_attention_mask=t_global,\n",
    "                       return_dict=True)\n",
    "\n",
    "        loss = result.loss\n",
    "\n",
    "        # Summing training loss to calculate AVG\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        #L2 regularization\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    total_eval_loss = 0\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        v_input_ids = batch[0].to(device)\n",
    "        v_input_mask = batch[1].to(device)\n",
    "        v_labels = batch[2].to(device)\n",
    "        v_global = batch[3].to(device)\n",
    "        \n",
    "\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            result = model(v_input_ids, \n",
    "                           attention_mask=v_input_mask,\n",
    "                           labels=v_labels,\n",
    "                           global_attention_mask=v_global,\n",
    "                           return_dict=True)\n",
    "\n",
    "        loss = result.loss\n",
    "            \n",
    "        # Summing the validation loss.\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pWb6XiMz4Dj7"
   },
   "outputs": [],
   "source": [
    "# This block saves the finetuned model\n",
    "\n",
    "output_dir = '/content/drive/MyDrive/Longformer_checkpoint/'\n",
    "\n",
    "model.save_pretrained(output_dir)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "background_execution": "on",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Copy of Longformer Finetuning.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
