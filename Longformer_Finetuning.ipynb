{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HEkSMyRunAj-"
   },
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rNOxyuD7nc1j"
   },
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import torch\n",
    "from google.colab import drive\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AdamW, \n",
    "    LongformerForSequenceClassification, \n",
    "    LongformerTokenizer, \n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\", exclude=[\n",
    "    \"ner\", \"tagger\", \"parser\", \"lemmatizer\", \"textcat\", \"attribute_ruler\"\n",
    "])\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Directories\n",
    "DATA_DIR = '/content/drive/MyDrive/MSMARCO/'  # Data directory\n",
    "MAIN_DIR = '/content/drive/MyDrive/'  # Main directory\n",
    "\n",
    "# GPU Configuration\n",
    "print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "DEVICE = torch.device(\"cuda\")  # To run the model and process the tensors on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5h90U53sorjE"
   },
   "outputs": [],
   "source": [
    "# Data preprocess block, this takes MSMARCO dataset which only has positive labels (qrels)\n",
    "# and assigns a random document to each query from the collection to be used as a negative label\n",
    "# It also adds query text and document text so this final file can be used without referencing the huge collection file\n",
    "# in the end we have a file with these fields columns=['qid', '0', 'docid', 'label', 'query_text','doc_text'])\n",
    "\n",
    "\n",
    "# Load datasets\n",
    "train_queries = pd.read_csv(os.path.join(data_dir, 'queries.doctrain.tsv'), sep='\\t', header=None, names=['qid', 'query_text'])\n",
    "train_queries_index = train_queries.set_index('qid')\n",
    "\n",
    "train_relations = pd.read_csv(os.path.join(data_dir, 'msmarco-doctrain-qrels.tsv'), sep=' ', header=None, names=['qid', '0', 'docid', 'label'])\n",
    "\n",
    "val_queries = pd.read_csv(os.path.join(data_dir, 'queries.docdev.tsv'), sep='\\t', header=None, names=['qid', 'query_text'])\n",
    "val_queries_index = val_queries.set_index('qid')\n",
    "\n",
    "val_relations = pd.read_csv(os.path.join(data_dir, 'msmarco-docdev-qrels.tsv'), sep=' ', header=None, names=['qid', '0', 'docid', 'label'])\n",
    "\n",
    "lookup = pd.read_csv(os.path.join(data_dir, 'msmarco-docs-lookup.tsv'), sep='\\t', header=None, names=['docid', 'trec_offset', 'tsv_offset'], usecols=['docid', 'trec_offset', 'tsv_offset'])\n",
    "lookup_list = lookup.values.tolist()\n",
    "\n",
    "collection = pd.read_csv(os.path.join(data_dir, 'msmarco-docs.tsv'), sep='\\t', header=None, names=['docid', 'url', 'title', 'doc_text'], index_col='docid')\n",
    "\n",
    "\n",
    "def negative_samples(queries, relations, queries_index):\n",
    "    negative_samples_list = []\n",
    "    for i in tqdm(range(len(queries))):\n",
    "        tmp_list = [queries[i][0], '0', random.sample(lookup_list, 1)[0][0], 0, queries[i][1]]\n",
    "        negative_samples_list.append(tmp_list)\n",
    "    for k in tqdm(range(len(relations))):\n",
    "        query_text = queries_index.loc[relations[k][0]].query_text\n",
    "        relations[k].append(query_text)\n",
    "    return negative_samples_list + relations\n",
    "\n",
    "\n",
    "def add_doc_text(dataset):\n",
    "    for i in tqdm(range(len(dataset))):\n",
    "        docid = dataset[i][2]\n",
    "        document = collection.loc[docid].doc_text\n",
    "        dataset[i].append(document)\n",
    "    return pd.DataFrame(dataset, columns=['qid', '0', 'docid', 'label', 'query_text', 'doc_text'])\n",
    "\n",
    "train_dataset = add_doc_text(negative_samples(train_queries.values.tolist(), train_relations.values.tolist(), train_queries_index))\n",
    "val_dataset = add_doc_text(negative_samples(val_queries.values.tolist(), val_relations.values.tolist(), val_queries_index))\n",
    "train_dataset.to_csv(os.path.join(data_dir, 'train_dataset.csv'))\n",
    "val_dataset.to_csv(os.path.join(data_dir, 'val_dataset.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nYLPpbeYSXDI"
   },
   "outputs": [],
   "source": [
    "# This block is to be used when a preprocessed dataset is already created (that has positive and negative labels)\n",
    "\n",
    "\n",
    "train_dataset = pd.read_csv(os.path.join(data_dir, 'train_dataset.csv'), header=0,\n",
    "                          names=['qid', '0', 'docid', 'label', 'query_text', 'doc_text'])\n",
    "\n",
    "val_dataset = pd.read_csv(os.path.join(data_dir, 'val_dataset.csv'), header=0,\n",
    "                          names=['qid', '0', 'docid', 'label', 'query_text', 'doc_text'])                         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ISV3U44BcOhH"
   },
   "outputs": [],
   "source": [
    "tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096') # Tokeizer initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q4IurB99cvb_"
   },
   "outputs": [],
   "source": [
    "# This function handels the toknizer and returns TensorDataset to be fed to the model\n",
    "\n",
    "def tokenize(dataset, labels):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    global_attention_mask = [] \n",
    "\n",
    "    for i in tqdm(range(len(dataset))):\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            str(dataset[i][4]),  # Query\n",
    "            str(dataset[i][5]),  # Document\n",
    "            add_special_tokens=True,  # Add '<s>' and '</s>'\n",
    "            max_length=1024,  # Pad & truncate all sentences.\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,  # Construct attention masks.\n",
    "            truncation='only_second',\n",
    "            return_tensors='pt',  # Return pytorch tensors.\n",
    "        )\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "        global_attention = [0] * 1024\n",
    "        range_with_CLS = len(nlp(dataset[i][4])) + 1\n",
    "        for j in range(range_with_CLS):\n",
    "            global_attention[j] = 1\n",
    "        global_attention_mask.append(torch.tensor([global_attention]))\n",
    "\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    global_attention_masks = torch.cat(global_attention_mask, dim=0)\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    return TensorDataset(input_ids, attention_masks, labels, global_attention_masks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9SjUgyM9im1T"
   },
   "outputs": [],
   "source": [
    "# This block calls the tokenizer defined above and saves the tensor datasets to .pt files,\n",
    "# so we do no have to go through this process every time we run the model\n",
    "\n",
    "\n",
    "\n",
    "# Prepare tensor datasets\n",
    "train_dataset_list = train_dataset.values.tolist() # Converting the dataframe to list (more efficient)\n",
    "val_dataset_list = val_dataset.values.tolist() # Converting the dataframe to list (more efficient)\n",
    "train_Labels = train_dataset.label.values # Extracting labels\n",
    "val_labels = val_dataset.label.values # Extracting labels\n",
    "\n",
    "train_dataset_tensor = tokenize(train_dataset_list, train_Labels) # Tokenization and creation of TensorDataset\n",
    "val_dataset_tensor = tokenize(val_dataset_list, val_labels) # Tokenization and creation of TensorDataset\n",
    "\n",
    "torch.save(train_dataset_tensor, os.path.join(MAIN_DIR, 'train_dataset_tensor.pt') # Saving TensorDataset to disk  \n",
    "torch.save(val_dataset_tensor, os.path.join(MAIN_DIR, 'val_dataset_tensor.pt') # Saving TensorDataset to disk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block is to be used when a TensorDataset is already created and saved to disk\n",
    "\n",
    "train_dataset_tensor = torch.load('/content/drive/MyDrive/Longformer_train_dataset_tensor.pt') # Training dataset, Tokenized and converted to tensors ..  Type is TensorDataset\n",
    "val_dataset_tensor = torch.load('/content/drive/MyDrive/Longformer_val_dataset_tensor.pt') # Validation dataset, Tokenized and converted to tensors ..  Type is TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IMsKeM-btxe6"
   },
   "outputs": [],
   "source": [
    "# DataLoader parameters\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# Create DataLoader for training set\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset_tensor,  # The training samples.\n",
    "    sampler=RandomSampler(train_dataset_tensor),  # Select batches randomly\n",
    "    batch_size=BATCH_SIZE  # Trains with this batch size.\n",
    ")\n",
    "\n",
    "# Create DataLoader for validation set\n",
    "validation_dataloader = DataLoader(\n",
    "    val_dataset_tensor,  # The validation samples.\n",
    "    sampler=SequentialSampler(val_dataset_tensor),  # Pull out batches sequentially.\n",
    "    batch_size=BATCH_SIZE  # Evaluate with this batch size.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nxwAwr-awCYe"
   },
   "outputs": [],
   "source": [
    "model = LongformerForSequenceClassification.from_pretrained('allenai/longformer-base-4096', num_labels = 2)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MLFzvaWJwyD6"
   },
   "outputs": [],
   "source": [
    "# This block defines the optimizer and the scheduler\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=3e-6)\n",
    "EPOCHS = 1\n",
    "total_steps = len(train_dataloader) * EPOCHS\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=2500,\n",
    "    num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XQeHCKr3eR0k"
   },
   "outputs": [],
   "source": [
    "for epoch_i in range(0, EPOCHS):\n",
    "    print(f\"\\n======== Epoch {epoch_i + 1} / {EPOCHS} ========\")\n",
    "    print('Training...')\n",
    "\n",
    "    t0 = time.time()\n",
    "    total_train_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if step % 100 == 0 and not step == 0:\n",
    "            print(f'  Batch {step:>5,}  of  {len(train_dataloader):>5,}.')\n",
    "\n",
    "        t_input_ids, t_input_mask, t_labels, t_global = (b.to(DEVICE) for b in batch)\n",
    "\n",
    "        \n",
    "        model.zero_grad()\n",
    "        result = model(\n",
    "            t_input_ids,\n",
    "            attention_mask=t_input_mask, \n",
    "            labels=t_labels,\n",
    "            global_attention_mask=t_global,\n",
    "            return_dict=True\n",
    "        )\n",
    "        loss = result.loss\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    print(f\"\\n  Average training loss: {avg_train_loss:.2f}\")\n",
    "\n",
    "    print(\"\\nRunning Validation...\")\n",
    "    t0 = time.time()\n",
    "    model.eval()\n",
    "    total_eval_loss = 0\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "\n",
    "        v_input_ids, v_input_mask, v_labels, v_global = (b.to(DEVICE) for b in batch)\n",
    "        with torch.no_grad():\n",
    "            result = model(\n",
    "                v_input_ids, \n",
    "                attention_mask=v_input_mask,\n",
    "                labels=v_labels,\n",
    "                global_attention_mask=v_global,\n",
    "                return_dict=True\n",
    "            )\n",
    "        loss = result.loss\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    print(f\"  Validation Loss: {avg_val_loss:.2f}\")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pWb6XiMz4Dj7"
   },
   "outputs": [],
   "source": [
    "# This block saves the finetuned model\n",
    "\n",
    "output_dir = '/content/drive/MyDrive/Longformer_checkpoint/'\n",
    "\n",
    "model.save_pretrained(output_dir)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "background_execution": "on",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Copy of Longformer Finetuning.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
